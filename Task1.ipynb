{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 02/09/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* re (for regular expression, included in Anaconda Python 3.6) \n",
    "* json (to convert given data into json format, included in Anaconda Python 3.6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the libraries used in Assignment task1\n",
    "import re    \n",
    "import json\n",
    "\n",
    "#Loading the file  \n",
    "with open('29416000.dat','r') as infile:\n",
    "    text = infile.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. Data Analysing or Visualization\n",
    "\n",
    "This section is done in the background to determine which is the most occuring attribute and also to determine different versions/patterns of the same attribute. \n",
    "\n",
    "<B>import pandas as pd</B>\n",
    "\n",
    "<B>attributes_df=pd.DataFrame(data=re.findall('[\\n]\\s*(_?[A-Za-z]+\\s*?[_]?[A-Za-z]+?)[:]',text)) </B>\n",
    "#The regular expression fetches all the Attributes in a file which are starting from Alphabets(uppercase or lowecase),Underscore,blank space and followed by ':' previous char should be a newline.\n",
    "\n",
    "<B>count=attributes_df[0].value_counts().rename('attritube_freq')</B>\n",
    "#This gives individual character frequency count.\n",
    "\n",
    "<B>attributes_df = attributes_df.merge(count.to_frame(),left_on=0,right_index=True)</B>\n",
    "#Merging it to form a new dataframe\n",
    "\n",
    "<B>unq=attributes_df[0].unique()</B>\n",
    "#unique attributes in a file which helped to determine the different versions of the attribute for instance, JOB_T and TITLE for description.\n",
    "\n",
    "<B>new_df=attributes_df[attributes_df.attritube_freq>=65]</B>\n",
    "#Neglecting all the attributes which have count less than 65(fixed by looking at the dataframe frequency values)  because those are the words picked up by the regular expression which are not exactly attributes(beacause the file contains the same pattern which are not attributes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.Program Flow (Json file creation)\n",
    "\n",
    "#### 3.1 Regular Expression Compilation using re.complie\n",
    "The block below compiles regular expressions pattern into regular expression object. It reduces the compilation time.\n",
    "\n",
    "Syntax:re.compile(pattern, flags=0)\n",
    "\n",
    "* Variations in Location attribute: LOCS,LOCATONS,LOC,JOB_LOC,_LOC,LOCATION <br/><br/>\n",
    "* Variations in Title attribute:_TTL,TITLES,JOB_T,title <br/><br/>\n",
    "* Variations in Description attribute: _description,DESCRIPTION,JOB_DESCRIPTION,JOB_DESC,JOb DESCRIPTION <br/><br/>\n",
    "* Variations in About company attribute attribute: _info,ABOUT COMPANY,ABOUT,about_comany,COMAPNYS_INFO <br/><br/>\n",
    "* Variations in Start Date attribute: DATES,DATE_START,START_DA,START DATE,start_date <br/><br/>\n",
    "* Variations in Qualifications attribute: qualifications,QUALIFS,QUALIFICATION,EQ_QUALS,REQUIRED QUALIFICATIONS,QUALIFICATIONS <br/><br/>\n",
    "* Variations in Application deadline attribute: APPLICATION_DEADL,APPLICATIONS_DL,DEADLINE,DEAD_LINE,deadline,DEADLINES <br/><br/>\n",
    "* Variations in Resposibility attribute: RESP,resposibilitiy,RESPOSIBILTIY, JOB RESPOSIBILTIES ,JOB_RESPS <br/><br/>\n",
    "* Variations in Procedure attribute: PROCEDURE,procedure,JOB_PROCS,JOB_PROC,PROCDEDURES <br/><br/>\n",
    "* Variations in Salary attribute: JOB_SAL,SALARY,salary,Salary <br/><br/>\n",
    "* Variations in Remuneration attribute: Remuneration, Remuneration.<br/><br/>\n",
    "\n",
    "\n",
    "All these Variations are complied by writing a regular expression for each which is shown in the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loc=re.compile('_?LOCS?A?T?I?O?N?S?:|JOB_LOC:') #Regex fetches all the locations, Similary for all the attributes\n",
    "Title=re.compile('_TTL:|TITLES?:|JOB_T|title:')\n",
    "Job=re.compile('J?O?B?\\s?_?DESCR?I?P?T?I?O?N?:|_description:|job_desc:',re.DOTALL)\n",
    "About=re.compile('ABOUT?\\s?C?O?M?P?A?N?Y?S?_?I?N?F?O?:|about_company|_info:|COMPANYS_INFO:',re.DOTALL)\n",
    "Date=re.compile('DATES?_?S?T?A?R?T?:|START\\s?_?D?A?T?E?:|start_date:')\n",
    "Qualify=re.compile('QUALIF?S?I?C?A?T?I?O?N?S?:|qualifications:|REQUIRED QUALIFICATIONS:|REQ_QUALS:')\n",
    "Deadline=re.compile('APPLICATION_DE?A?D?L?:|DEAD_?LINES?:|deadline:')\n",
    "Resp=re.compile('J?O?B?_?\\s?RESPO?N?S?I?B?I?L?I?T?Y?I?E?S?:|responsibilities:')\n",
    "Proc=re.compile('PROCEDURES?:|JOB_PROCS?:',re.IGNORECASE)\n",
    "Salary=re.compile('J?O?B?_?SALA?R?Y?:|\\nSalary:',re.IGNORECASE)\n",
    "Remuneration=re.compile('REMUNERATION:',re.IGNORECASE)\n",
    "remove=re.compile('REMUNERATION/|START DATE/|OPEN TO/')#used to remove key which has '/'at the end (as it contains no value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking the count of each  Attributes in all the job postings before modification\n",
    "\n",
    "re.findall - Returns all the non overlapping matches of the passed string as a list. \n",
    "\n",
    "Syntax: re.findall(pattern, string)Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all the patterns\n",
    "location_C=re.findall(Loc,text)\n",
    "Title_c=re.findall(Title,text)\n",
    "Job_c=re.findall(Job,text)\n",
    "About_c=re.findall(About,text)\n",
    "Date_c=re.findall(Date,text)\n",
    "Qualify_c=re.findall(Qualify,text)\n",
    "Deadline_c=re.findall(Deadline,text)\n",
    "Resp_c=re.findall(Resp,text)\n",
    "Proc_c=re.findall(Proc,text)\n",
    "Salary_c=re.findall(Salary,text)\n",
    "Remuneration_c=re.findall(Remuneration,text)\n",
    "Id_c=re.findall('ID:',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of IDs: 30503\n",
      "Total number of Locations: 28329\n",
      "Total number of Title: 28376\n",
      "Total number of Job Description: 28684\n",
      "Total number of About Company: 28252\n",
      "Total number of Start Date: 28410\n",
      "Total number of Qualification: 28495\n",
      "Total number of Procedure: 28377\n",
      "Total number of Remuneration: 10025\n",
      "Total number of Salary: 18462\n",
      "Total number of Resposibility: 28411\n",
      "Total number of Deadline: 28334\n"
     ]
    }
   ],
   "source": [
    "#printing the coount of each attribute before modification.\n",
    "print('Total number of IDs:',len(Id_c))\n",
    "print('Total number of Locations:',len(location_C))\n",
    "print('Total number of Title:',len(Title_c))\n",
    "print('Total number of Job Description:',len(Job_c))\n",
    "print('Total number of About Company:',len(About_c))\n",
    "print('Total number of Start Date:',len(Date_c))\n",
    "print('Total number of Qualification:',len(Qualify_c))\n",
    "print('Total number of Procedure:',len(Proc_c))\n",
    "print('Total number of Remuneration:',len(Remuneration_c))\n",
    "print('Total number of Salary:',len(Salary_c))\n",
    "print('Total number of Resposibility:',len(Resp_c))\n",
    "print('Total number of Deadline:',len(Deadline_c))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.2. Replacing the file with the common key attribute \n",
    "Replaces the string which matches the pattern compiled in the 3.1 section.The block below replaces every pattern matched with a common string.\n",
    "\n",
    "Syntax:re.sub(pattern, repl, string, count=0, flags=0)\n",
    "\n",
    "<font size=3 color=\"blue\">Why replacing with commom attribute ? </font><br/>\n",
    "\n",
    "I am doing this because all the different versions of the attribute have to be normalised before passing it to json or xml. Instead of replacing it in the later stages, I felt replacing in the beginning would be easier to fetch values of each attribute. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with the attributes with common names\n",
    "modified_file=re.sub(Loc,'LOCATION:',text)\n",
    "modified_file=re.sub(Title,'TITLE:',modified_file)\n",
    "modified_file=re.sub(About,r'ABOUT:',modified_file)\n",
    "modified_file=re.sub(Job,r'DESCRIPTON:',modified_file)\n",
    "modified_file=re.sub(Date,r'STARTDATE:',modified_file)\n",
    "modified_file=re.sub(Qualify,r'QUALIFICATION:',modified_file)\n",
    "modified_file=re.sub(Deadline,r'DEADLINE:',modified_file)\n",
    "modified_file=re.sub(Resp,r'RESPONSIBILITY:',modified_file)\n",
    "modified_file=re.sub(Proc,r'PROCEDURE:',modified_file)\n",
    "modified_file=re.sub(Salary,r'SALARY:',modified_file)\n",
    "modified_file=re.sub(Remuneration,r'REMUNERATION:',modified_file)\n",
    "modified_file=re.sub(remove,r'',modified_file)# removing the attributes which deosn't have value.\n",
    "modified_file=re.sub('REMUNERATION:','SALARY:',modified_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fetching the counts after replacing the keywords\n",
    "\n",
    "Fetching the counts for each attribute after modification and ensuring the counts before and after the replacing matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_C=re.findall('LOCATION:',modified_file)\n",
    "Title_c=re.findall('TITLE:',modified_file)\n",
    "Job_c=re.findall('DESCRIPTON:',modified_file)\n",
    "About_c=re.findall('ABOUT:',modified_file)\n",
    "Date_c=re.findall('STARTDATE:',modified_file)\n",
    "Qualify_c=re.findall('QUALIFICATION:',modified_file)\n",
    "Deadline_c=re.findall('DEADLINE:',modified_file)\n",
    "Resp_c=re.findall('RESPONSIBILITY:',modified_file)\n",
    "Proc_c=re.findall('PROCEDURE:',modified_file)\n",
    "Salary_c=re.findall('SALARY:',modified_file)\n",
    "Id_c=re.findall('ID:',modified_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of IDs: 30503\n",
      "Total number of Locations: 28329\n",
      "Total number of Title: 28376\n",
      "Total number of Job Description: 28684\n",
      "Total number of About Company: 28252\n",
      "Total number of Start Date: 28410\n",
      "Total number of Qualification: 28495\n",
      "Total number of Procedure: 28377\n",
      "Total number of Salary: 28487\n",
      "Total number of Resposibility: 28411\n",
      "Total number of Deadline: 28334\n"
     ]
    }
   ],
   "source": [
    "print('Total number of IDs:',len(Id_c))\n",
    "print('Total number of Locations:',len(location_C))\n",
    "print('Total number of Title:',len(Title_c))\n",
    "print('Total number of Job Description:',len(Job_c))\n",
    "print('Total number of About Company:',len(About_c))\n",
    "print('Total number of Start Date:',len(Date_c))\n",
    "print('Total number of Qualification:',len(Qualify_c))\n",
    "print('Total number of Procedure:',len(Proc_c))\n",
    "print('Total number of Salary:',len(Salary_c))\n",
    "print('Total number of Resposibility:',len(Resp_c))\n",
    "print('Total number of Deadline:',len(Deadline_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Splitting the file based on the identified delimitter<br/>\n",
    "\n",
    "\n",
    "<font size=3 color=\"blue\">Why we need to split ?</font>\n",
    "\n",
    "Splitting would be necessary as we need to capture information for each job posting.  \n",
    "\n",
    "The delimitter is identified as `'------------------------------'` and deleting the last index as at the end there is also delimitter which eventually gives one empty string at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_modified_file=modified_file.split('------------------------------')# splits and stores it as a list os job postings\n",
    "del split_modified_file[len(split_modified_file)-1] # deleting last index "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Extracting All the Attributes\n",
    "\n",
    "re.findall method extracts the pattern given as a input and return a list. If more than one group is captured it returns as list of tuples.  \n",
    "\n",
    "Syntax: re.findall(pattern, string, flags=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict=[] # stores list of dictionaries \n",
    "for each in split_modified_file: # running a loop for each Job postings\n",
    "    job_dic={}   # initializing the empty dictionary\n",
    "    keys=re.findall(r'([A-Z]+):',each) # extract all keys matching the pattern\n",
    "    for i in range(len(keys)):#running a loop to extract values between two keys               \n",
    "        if i<len(keys)-1: # condtion is put to avoid out of range, as 'i+1' goes out of index when it reached last attribute\n",
    "            job_dic[keys[i]]=re.search(keys[i]+':'+'(.*?)'+keys[i+1],each,re.DOTALL).group(1)# fetches key,value pair dynamically.\n",
    "        else:\n",
    "            job_dic[keys[i]]=re.search(keys[i]+':'+'(.*)',each,re.DOTALL).group(1)# fetches last attribute key,value pair\n",
    "    list_dict.append(job_dic) # finally appends to list of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 Handling Inner divisions of attributes\n",
    "\n",
    "As observed RESPONSIBILITY,DESCRIPTION,QUALIFICATION have subparts and hence needs division accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in list_dict:\n",
    "    for i,j in each.items():\n",
    "        if len(re.findall(r'(\\n-.*?\\s*;)',j)) !=0: # checks if any attribute has subdivion by checking if there are bullet points'-' and ends with ';' \n",
    "            if i=='RESPONSIBILITY':\n",
    "                resp=j.replace('-','').strip().split(';')# spiltting by ';' ,replacing '-' by null and stripping blank spaces.\n",
    "                obj_resp={'Responsibility':resp} # Assigning it to dictionary\n",
    "                each[i]=obj_resp # assigning back to attribute.\n",
    "            elif i=='DESCRIPTION':# The above steps are repeated for description and qualification\n",
    "                desc=j.replace('-','').strip().split(';')\n",
    "                obj_desc={'Description':desc}\n",
    "                each[i]=obj_desc\n",
    "            elif i=='QUALIFICATION':\n",
    "                qualf=j.replace('-','').strip().split(';')\n",
    "                obj_qualf={'Description':qualf}\n",
    "                each[i]=obj_qualf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 Creating Dictionary objects \n",
    "\n",
    "This helps in creating Dictionaries inside Dictionary which is easier to dump into json method to covert into json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_listing={'Listing':list_dict} # Adding one dictionary to other\n",
    "obj_listings={'Listings':obj_listing}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7 Writing the output to json file\n",
    "\n",
    "Its uses json.dump method which converts dictionary of dictionary to json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('29416000.json', 'w') as out:\n",
    "    json.dump(obj_listings, out) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Program Flow (Xml file creation)\n",
    "\n",
    "The program takes input as dictionary of dictionaries which is created in the previous section and loops over each dictionary and appends angular brackets{<''>... </>} for each key,value pair.\n",
    "\n",
    "I did not wanted to hard code values like 'required qualications' etc and hence using the same key for the headings inside   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[] # empty list to store each output, Looping over each dictionaries and attaching angular brackets to Attributes and Values\n",
    "for i,j in obj_listings.items():    \n",
    "    result.append('<'+i+'>')\n",
    "    for x,y in j.items():\n",
    "        for _ in y:\n",
    "            result.append('<'+x+'>')\n",
    "            for k,v  in _.items():              \n",
    "                temp=type(v)\n",
    "                if temp!=dict:                   \n",
    "                   result.append('<'+k+'>'+v+'</'+k+'>') \n",
    "                else:\n",
    "                    for m,n in v.items():\n",
    "                        result.append('<'+m+'>')\n",
    "                        for each in n:                            \n",
    "                            result.append('<'+m+'>'+each+'</'+m+'>')\n",
    "                        result.append('</'+m+'>')\n",
    "            result.append('</'+x+'>')       \n",
    "    result.append('</'+i+'>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Joining the result list into a string. \n",
    "File contains '\\n' and hence replacing it with null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalxml=''.join(result) # converts list into string\n",
    "replace=finalxml.replace('\\n','') # when converted it adds '\\n' and hence removing it by using replace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Writing the output to XML file\n",
    "\n",
    "Its uses file.write method which converts string into .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84138348"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "File = open('29416000.xml','w') \n",
    "File.write(str(replace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that ID is identifier and it is present in all the Job postings.Hence Normalizing all attributes with respect to ID which we can infer the below following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percetage of IDs: 100.0\n",
      "Percetage of Locations: 92.87283218044126\n",
      "Percetage of Title: 93.02691538537194\n",
      "Percetage of Job Description: 94.03665213257713\n",
      "Percetage of About Company: 92.62039799363997\n",
      "Percetage of Start Date: 93.13837983149197\n",
      "Percetage of Qualification: 93.41704094679211\n",
      "Percetage of Procedure: 93.03019375143428\n",
      "Percetage of Salary: 93.39081401829328\n",
      "Percetage of Resposibility: 93.14165819755434\n",
      "Percetage of Deadline: 92.88922401075304\n"
     ]
    }
   ],
   "source": [
    "#Printing the percentages of Each attribute\n",
    "print('Percetage of IDs:',len(Id_c)/len(Id_c)*100)\n",
    "print('Percetage of Locations:',len(location_C)/len(Id_c)*100)\n",
    "print('Percetage of Title:',len(Title_c)/len(Id_c)*100)\n",
    "print('Percetage of Job Description:',len(Job_c)/len(Id_c)*100)\n",
    "print('Percetage of About Company:',len(About_c)/len(Id_c)*100)\n",
    "print('Percetage of Start Date:',len(Date_c)/len(Id_c)*100)\n",
    "print('Percetage of Qualification:',len(Qualify_c)/len(Id_c)*100)\n",
    "print('Percetage of Procedure:',len(Proc_c)/len(Id_c)*100)\n",
    "print('Percetage of Salary:',len(Salary_c)/len(Id_c)*100)\n",
    "print('Percetage of Resposibility:',len(Resp_c)/len(Id_c)*100)\n",
    "print('Percetage of Deadline:',len(Deadline_c)/len(Id_c)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "The provided Job Postings Dataset contains <b> 30503 </b>different job postings. I have identified totally <b>11</b> key attributes for each job posting. As per the statistics shown there may be missing key attribitues in each job posting.The precentage of each attribute present in whole text file provided is as shown below.\n",
    "\n",
    "* Percetage of IDs: 100.0\n",
    "* Percetage of Locations: 92.87283218044126\n",
    "* Percetage of Title: 93.02691538537194\n",
    "* Percetage of Job Description: 94.03665213257713\n",
    "* Percetage of About Company: 92.62039799363997\n",
    "* Percetage of Start Date: 93.13837983149197\n",
    "* Percetage of Qualification: 93.41704094679211\n",
    "* Percetage of Procedure: 93.03019375143428\n",
    "* Percetage of Salary: 93.39081401829328\n",
    "* Percetage of Resposibility: 93.14165819755434\n",
    "* Percetage of Deadline: 92.88922401075304\n",
    "\n",
    "The steps performed in Data Extraction and Manipulation are as follows:\n",
    "\n",
    "1. Writing a regex to extract Attributes\n",
    "2. Analysing the attributes by its frequency and Uniqueness to get different versions of attributes.\n",
    "3. Writing a regex for each attribute and replacing it with single word.\n",
    "4. Extracting the text between two Atrributes.\n",
    "5. Creating json objects to finally dump as json file.\n",
    "6. Constructing XML strings by adding tags with the help of already created json object.\n",
    "7. Writing to file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. References \n",
    "\n",
    "* https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\n",
    " \n",
    "* https://docs.python.org/3/library/re.html\n",
    "\n",
    "* https://docs.python.org/3/library/json.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
