{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date: 02/09/2018\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3.6.0 and Anaconda 4.3.0 (64-bit)\n",
    "\n",
    "Libraries used:<br/>\n",
    "\n",
    "* nltk.data (for sentence detector)\n",
    "* re (for regular expressions)\n",
    "* from nltk.tokenize ( RegexpTokenizer for tokenization )\n",
    "* from nltk.stem  (PorterStemmer for Stemming)\n",
    "* from nltk.util  (ngrams for bigrams)\n",
    "* from nltk.probability (for frequency Distribution) \n",
    "* from itertools chain (to join multiple dictionary values into one) \n",
    "* from nltk.tokenize (MWETokenizer to join bigrams and retokenize)\n",
    "* from sklearn.feature_extraction.text CountVectorizer (Vectorizing in the format)\n",
    "\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "This assignment comprises the execution of different text processing and analysis tasks applied to Resumes . There are a total of 250 resumes and files are named `resume_(n).txt1`(where n is an integer). The required tasks are the following:\n",
    "\n",
    "1. Tokenize each file.\n",
    "2. Normalize the tokens.\n",
    "3. List out top 200 bigrams.\n",
    "4. Remove all the stop words and rare words with the threshold mentioned to 98% and 2%.\n",
    "5. Stemming each token.\n",
    "6. Vectorizing and writing to a file.\n",
    "\n",
    "More details for each task will be given in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "import re \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.probability import *\n",
    "from itertools import chain\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first step, all resume names are loaded into list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching individual Resumes\n",
    "resume='100 171 244 716 496 336 293 326 200 510 41 616 478 473 859 159 586 293 275 424 3 611 68 577 757 30 594 9 783 822 135 820 272 505 487 24 313 188 139 220 137 74 389 710 15 704 726 703 193 499 317 638 95 464 425 428 701 657 724 786 681 344 189 716 298 25 509 39 26 269 223 738 593 714 449 242 81 687 155 487 661 464 757 430 185 781 421 796 618 842 649 254 568 562 492 99 782 705 105 603 692 272 861 691 287 809 114 254 301 783 418 263 107 341 833 804 361 403 114 685 313 256 236 543 446 12 798 505 28 640 536 169 255 576 257 769 499 443 596 734 445 440 689 580 299 412 270 124 555 360 396 361 566 555 409 686 162 829 228 176 310 617 59 55 203 238 52 563 289 345 70 215 576 388 198 32 295 341 318 534 8 622 200 793 132 827 704 431 280 596 107 551 32 164 771 479 166 542 451 399 185 382 515 347 85 218 210 658 449 459 299 258 132 557 764 280 489 46 388 24 655 237 849 535 435 650 48 798 544 743 483 614 493 569 727 454 10 34 100 782 798 143 778 240 194 237 224 11 538 540'\n",
    "resume=resume.split(' ') # Splitting by space\n",
    "resumeset=set(resume)\n",
    "resume_names=[]\n",
    "for each in resumeset:\n",
    "    resume_names.append('resume_('+each+').txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total Number of resumes used for analysis is 217 and list provided is 250\n"
     ]
    }
   ],
   "source": [
    "print('The total Number of resumes used for analysis is', len(resume_names), 'and list provided is',len(resume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Resume list provided has duplicate files in it and hence considered unique resumes for the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n"
     ]
    }
   ],
   "source": [
    "#Loading StopWords\n",
    "stopwords_file=open('stopwords_en.txt',\"r\",encoding=\"utf8\")\n",
    "stopwords=stopwords_file.read().split('\\n')\n",
    "print(len(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading stop words from the stopword list provided. It contains 571 lowercase stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pre-processing and  Tokenization\n",
    "\n",
    "#### Pre-processing\n",
    "\n",
    "As part of pre-processing of the resumes three tasks are carried out. They are\n",
    "1. Removing all the bullet points and junk characters using regex  <font size=3 color=\"green\"> r'[^!\"' +\"#$%&'\"+'()*+,./:;<=>?@[\\]^_`{|}~\\sA-Za-z0-9]'</font> is used.<br/>This regex pattern matches bullet points(inclusive of '-') and other junk characters and replaces with nulls. <br/>\n",
    "2. Sentence Segmentation (Sentence Boundary Detection) <br/>It is the method of text processing where sentences are converted into tokens. The Assignment uses `nltk.data.load('tokenizers/punkt/english.pickle')` tokenizer to detect sentences ending with full stop or Question mark or Exclamation mark.<br/>\n",
    "\n",
    "3. Case Normalization\n",
    "It is the process of converting the uppercase character in to lowercase charaters. As given in the assignment specifications\n",
    "the first word of all the sentences are normalized to lowercase charaters. <br/> \n",
    "\n",
    "<font size=3 color=\"blue\"> why do we do case normalization?</font><br/>\n",
    "\n",
    "Case Normalization is done to reduce the number of words in the final vocabulary list which would be helpfull at the later stages for analysis. For instance, ‘qualifications’ and ‘Qualifications’ mean the same when context is Resume. Hence Normalizing these kind of words will not result in loss of information.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "Tokenization is breaking up of sentence or paragraph into meaningful pieces such as words, keywords or phrases etc. As per assignment we have been provided a regular expression to split the text into tokens.\n",
    "<font size=3 color=\"blue\"> Why Tokenization?</font><br/>\n",
    "Tokenization is fundamental in all text processing algorithms. It is comparatively easy to do analysis when we split large chunks of data into smaller units.For instance analysis of frequencies of each word , sentiment analysis stylometric analysis etc would be easier if there are tokenised.\n",
    "\n",
    "\n",
    "The above mentioned preprocessing and tokenization are defined in one function called `tokenizeRawData` in the program. It takes each resume replaces junk values with nulls , segmentises the sentences , coverts the  first word of every sentence into lower case and at the end returns tokens and resume_name where tokenization is based on the given regular expression `r\"\\w+(?:[-']\\w+)?\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def tokenizeRawData(resume_id):  \n",
    "    fopen=open(resume_id,\"r\",encoding=\"utf8\")\n",
    "    text=fopen.read() \n",
    "    text_without_junk=''\n",
    "    reg=('[\\n][^!\"' +\"#$%&'\"+'()*+,-./:;<=>?@[\\]^_`{|}~\\sA-Za-z0-9]')# regex to remove bullet points and junk characters\n",
    "    text_without_junk+=re.sub(reg,'',text)\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle') # detecting sentences \n",
    "    sentences = sent_detector.tokenize(text_without_junk.strip())\n",
    "    lower_text='' # asssign sentence back to string after doing lowercase \n",
    "    for sent in sentences: \n",
    "        lower_funct = lambda word: word.group(1).lower() # lamda function to lower first word in a sentence.\n",
    "        lower_text+=re.sub(r'(^\\w+)', lower_funct, sent)\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+(?:[-']\\w+)?\") # tokenizing \n",
    "    tokenised_file=tokenizer.tokenize(lower_text) \n",
    "    return (resume_id, tokenised_file) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'CHIN',\n",
       " 'Kwok',\n",
       " 'Ho',\n",
       " 'Mobile',\n",
       " '852-5347',\n",
       " '8575',\n",
       " 'E-mail',\n",
       " 'chinkhthomas',\n",
       " 'gmail',\n",
       " 'com',\n",
       " 'Education',\n",
       " 'The',\n",
       " 'Hong',\n",
       " 'Kong',\n",
       " 'University',\n",
       " 'of',\n",
       " 'Science',\n",
       " 'and',\n",
       " 'Technology',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'BBA',\n",
       " 'in',\n",
       " 'Finance',\n",
       " 'and',\n",
       " 'Professional',\n",
       " 'Accounting',\n",
       " 'Second',\n",
       " 'Class',\n",
       " 'Honors',\n",
       " 'Division',\n",
       " 'I',\n",
       " 'Work',\n",
       " 'Experience',\n",
       " 'BOCI-Prudential',\n",
       " 'Trustee',\n",
       " 'Limited',\n",
       " 'Finance',\n",
       " 'Department',\n",
       " 'Senior',\n",
       " 'Fund',\n",
       " 'Accountant',\n",
       " 'Assistant',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'Valuated',\n",
       " 'monthly',\n",
       " 'Cayman',\n",
       " 'fund',\n",
       " 'SFC',\n",
       " 'funds',\n",
       " 'RQFII',\n",
       " 'and',\n",
       " 'QDII',\n",
       " 'funds',\n",
       " 'holding',\n",
       " 'different',\n",
       " 'types',\n",
       " 'of',\n",
       " 'financial',\n",
       " 'Sep',\n",
       " '2017',\n",
       " 'instruments',\n",
       " 'including',\n",
       " 'but',\n",
       " 'not',\n",
       " 'limited',\n",
       " 'to',\n",
       " 'stocks',\n",
       " 'options',\n",
       " 'futures',\n",
       " 'warrants',\n",
       " 'Cooperated',\n",
       " 'with',\n",
       " 'other',\n",
       " 'teammates',\n",
       " 'involving',\n",
       " 'trade',\n",
       " 'settlements',\n",
       " 'corporate',\n",
       " 'actions',\n",
       " 'and',\n",
       " 'price',\n",
       " 'movements',\n",
       " 'by',\n",
       " 'reconciling',\n",
       " 'with',\n",
       " 'Bloomberg',\n",
       " 'and',\n",
       " 'other',\n",
       " 'credible',\n",
       " 'sources',\n",
       " 'Coordinated',\n",
       " 'with',\n",
       " 'fund',\n",
       " 'managers',\n",
       " 'custodians',\n",
       " 'and',\n",
       " 'bankers',\n",
       " 'to',\n",
       " 'resolve',\n",
       " 'valuation',\n",
       " 'and',\n",
       " 'fund',\n",
       " 'setup',\n",
       " 'issues',\n",
       " 'Monitored',\n",
       " 'investment',\n",
       " 'position',\n",
       " 'margin',\n",
       " 'requirement',\n",
       " 'cash',\n",
       " 'flow',\n",
       " 'and',\n",
       " 'investment',\n",
       " 'compliance',\n",
       " 'Prepared',\n",
       " 'for',\n",
       " 'management',\n",
       " 'accounts',\n",
       " 'and',\n",
       " 'audit',\n",
       " 'queries',\n",
       " 'Coached',\n",
       " 'junior',\n",
       " 'colleagues',\n",
       " 'in',\n",
       " 'new',\n",
       " 'job',\n",
       " 'duties',\n",
       " 'and',\n",
       " 'job',\n",
       " 'rotations',\n",
       " 'Steven',\n",
       " 'Cheung',\n",
       " 'Co',\n",
       " 'audit',\n",
       " 'Trainee',\n",
       " 'Dec',\n",
       " '2014',\n",
       " 'Performed',\n",
       " 'audit',\n",
       " 'fieldwork',\n",
       " 'and',\n",
       " 'constructed',\n",
       " 'working',\n",
       " 'papers',\n",
       " 'from',\n",
       " 'the',\n",
       " 'data',\n",
       " 'received',\n",
       " 'in',\n",
       " 'clients',\n",
       " 'companies',\n",
       " 'Jan',\n",
       " '2015',\n",
       " 'HKUST',\n",
       " 'Department',\n",
       " 'of',\n",
       " 'Finance',\n",
       " 'Finance',\n",
       " 'Research',\n",
       " 'Assistant',\n",
       " 'Part-time',\n",
       " 'Apr-May',\n",
       " 'Assisted',\n",
       " 'a',\n",
       " 'finance',\n",
       " 'professor',\n",
       " 'in',\n",
       " 'collecting',\n",
       " 'and',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " 'necessary',\n",
       " 'for',\n",
       " 'his',\n",
       " 'research',\n",
       " 'projects',\n",
       " 'in',\n",
       " '2014',\n",
       " 'related',\n",
       " 'to',\n",
       " '100',\n",
       " 'targeted',\n",
       " 'family',\n",
       " 'firms',\n",
       " 'from',\n",
       " \"SEC's\",\n",
       " 'archives',\n",
       " 'Consolidated',\n",
       " 'and',\n",
       " 'analyzed',\n",
       " 'the',\n",
       " 'relevant',\n",
       " 'data',\n",
       " 'with',\n",
       " 'Excel',\n",
       " 'Extra-Curricular',\n",
       " 'Activities',\n",
       " 'EY',\n",
       " 'Young',\n",
       " 'Tax',\n",
       " 'Professional',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Year',\n",
       " '2015',\n",
       " 'organized',\n",
       " 'by',\n",
       " 'EY',\n",
       " 'Finalist',\n",
       " 'Mar',\n",
       " '2015',\n",
       " 'Was',\n",
       " 'examined',\n",
       " 'on',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fundamentals',\n",
       " 'of',\n",
       " 'international',\n",
       " 'tax',\n",
       " 'and',\n",
       " 'debated',\n",
       " 'on',\n",
       " 'a',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'issue',\n",
       " 'ETF',\n",
       " 'Stock',\n",
       " 'Investment',\n",
       " 'Simulation',\n",
       " 'Game',\n",
       " '2014',\n",
       " 'co-organized',\n",
       " 'by',\n",
       " 'HKEX',\n",
       " 'and',\n",
       " '23',\n",
       " 'sponsors',\n",
       " 'Nov',\n",
       " '2014',\n",
       " 'Stimulated',\n",
       " 'investment',\n",
       " 'in',\n",
       " 'designated',\n",
       " 'Exchange',\n",
       " 'Traded',\n",
       " 'Funds',\n",
       " 'ETFs',\n",
       " 'listed',\n",
       " 'and',\n",
       " 'traded',\n",
       " 'on',\n",
       " 'SEHK',\n",
       " 'and',\n",
       " 'stocks',\n",
       " 'included',\n",
       " 'in',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'Index',\n",
       " 'HSI',\n",
       " 'and',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'China',\n",
       " 'Enterprises',\n",
       " 'Index',\n",
       " 'HKSCI',\n",
       " 'Allocated',\n",
       " 'investment',\n",
       " 'in',\n",
       " 'the',\n",
       " 'prescribed',\n",
       " 'ratio',\n",
       " 'and',\n",
       " 'required',\n",
       " 'to',\n",
       " 're-build',\n",
       " 'portfolio',\n",
       " 'every',\n",
       " 'week',\n",
       " 'for',\n",
       " '4',\n",
       " 'weeks',\n",
       " 'Gained',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'into',\n",
       " 'ETF',\n",
       " 'and',\n",
       " 'stock',\n",
       " 'trading',\n",
       " 'by',\n",
       " 'simulation',\n",
       " 'Corporate',\n",
       " 'Governance',\n",
       " 'Paper',\n",
       " 'Competition',\n",
       " 'and',\n",
       " 'Presentation',\n",
       " 'Award',\n",
       " '2014',\n",
       " 'organized',\n",
       " 'by',\n",
       " 'HKICS',\n",
       " 'Finalist',\n",
       " 'Sep',\n",
       " '2014',\n",
       " 'Wrote',\n",
       " 'a',\n",
       " 'paper',\n",
       " 'and',\n",
       " 'gave',\n",
       " 'a',\n",
       " 'presentation',\n",
       " 'before',\n",
       " 'a',\n",
       " 'panel',\n",
       " 'of',\n",
       " 'judges',\n",
       " 'about',\n",
       " 'corporate',\n",
       " 'governance',\n",
       " 'according',\n",
       " 'to',\n",
       " 'the',\n",
       " 'given',\n",
       " 'theme',\n",
       " 'Changing',\n",
       " 'Rules',\n",
       " 'Changing',\n",
       " 'Roles',\n",
       " 'Managing',\n",
       " 'It',\n",
       " 'All',\n",
       " 'Navigate',\n",
       " 'My',\n",
       " 'Financial',\n",
       " 'Future',\n",
       " 'Case',\n",
       " 'Competition',\n",
       " 'organized',\n",
       " 'by',\n",
       " 'AIESEC',\n",
       " 'Hong',\n",
       " 'Kong',\n",
       " 'VISA',\n",
       " 'Finalist',\n",
       " 'Jun',\n",
       " '2013',\n",
       " 'Led',\n",
       " 'and',\n",
       " 'coordinated',\n",
       " 'the',\n",
       " 'team',\n",
       " 'and',\n",
       " 'wrote',\n",
       " 'a',\n",
       " 'proposal',\n",
       " 'aiming',\n",
       " 'at',\n",
       " 'improving',\n",
       " 'the',\n",
       " 'financial',\n",
       " 'knowledge',\n",
       " 'and',\n",
       " 'related',\n",
       " 'management',\n",
       " 'skills',\n",
       " 'of',\n",
       " 'senior',\n",
       " 'secondary',\n",
       " 'school',\n",
       " 'students',\n",
       " 'Made',\n",
       " 'a',\n",
       " 'video',\n",
       " 'illustrating',\n",
       " 'how',\n",
       " 'the',\n",
       " 'proposal',\n",
       " 'can',\n",
       " 'be',\n",
       " 'worked',\n",
       " 'out',\n",
       " 'with',\n",
       " 'Visual',\n",
       " 'Basic',\n",
       " 'Professional',\n",
       " 'Qualification',\n",
       " 'CFA',\n",
       " 'Level',\n",
       " 'II',\n",
       " 'Candidate',\n",
       " 'HKSI',\n",
       " 'LE',\n",
       " 'Paper',\n",
       " '1',\n",
       " 'HKICPA',\n",
       " 'QP',\n",
       " 'Paper',\n",
       " 'B',\n",
       " 'C',\n",
       " 'Languages',\n",
       " 'and',\n",
       " 'Computer',\n",
       " 'Skills',\n",
       " 'Knowledge',\n",
       " 'in',\n",
       " 'Bloomberg',\n",
       " 'and',\n",
       " 'proficient',\n",
       " 'in',\n",
       " 'Excel',\n",
       " 'and',\n",
       " 'Word',\n",
       " 'Proficient',\n",
       " 'in',\n",
       " 'English',\n",
       " 'and',\n",
       " 'Mandarin',\n",
       " 'Native',\n",
       " 'in',\n",
       " 'Cantonese',\n",
       " 'Current',\n",
       " 'salary',\n",
       " 'HKD',\n",
       " '14',\n",
       " '000',\n",
       " 'Expected',\n",
       " 'Salary',\n",
       " 'HKD',\n",
       " '18',\n",
       " '000']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating Dictionary of which includes resume name and tokens of each resume \n",
    "tokenized_resumes =  dict(tokenizeRawData(each_file) for each_file in resume_names)\n",
    "tokenized_resumes['resume_(100).txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After pre-processing and tokenization\n",
      "Length of word list:  142975\n",
      "Length of vocabulary list:  16594\n",
      "lexical diversity:  8.616066047969145\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(tokenized_resumes.values())) # fetching list of all tokens from all resumes\n",
    "vocab = set(words) #Vocabulary \n",
    "lexical_diversity = len(words)/len(vocab) # Calculating Lexical Diversity which means on an Average how many times a word is repeated.\n",
    "print('After pre-processing and tokenization')\n",
    "print('Length of word list: ',len(words))\n",
    "print('Length of vocabulary list: ',len(vocab))\n",
    "print('lexical diversity: ',lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.StopWords Removal \n",
    "\n",
    "Stopwords in English are more frequently used words. For instance, articles, prepositions etc;<br/>\n",
    "\n",
    "<font size=3 color=\"blue\">Why are we removing these words? and why at this stage?</font>\n",
    "\n",
    "As definition says they are frequent words which implies they will be present in almost all the resumes and hence keeping these words will not help in analysis.<br/>\n",
    "\n",
    "Stopwords removal has to be done in the earlier stages. It is best to do before stemming because some stopwords can be stemmed by stemmer and cannot to filtered later.This may end up in false analysis. For instance \"was\" stems into \"wa\", \"alone\" becomes \"alon\" by porter stemmer.\n",
    "\n",
    "On the other hand we can stem stopwards too and filer it later. By this approach we land up in stemmed bigrams which will not be meaningfull. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'CHIN',\n",
       " 'Kwok',\n",
       " 'Ho',\n",
       " 'Mobile',\n",
       " '852-5347',\n",
       " '8575',\n",
       " 'E-mail',\n",
       " 'chinkhthomas',\n",
       " 'gmail',\n",
       " 'Education',\n",
       " 'The',\n",
       " 'Hong',\n",
       " 'Kong',\n",
       " 'University',\n",
       " 'Science',\n",
       " 'Technology',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'BBA',\n",
       " 'Finance',\n",
       " 'Professional',\n",
       " 'Accounting',\n",
       " 'Second',\n",
       " 'Class',\n",
       " 'Honors',\n",
       " 'Division',\n",
       " 'I',\n",
       " 'Work',\n",
       " 'Experience',\n",
       " 'BOCI-Prudential',\n",
       " 'Trustee',\n",
       " 'Limited',\n",
       " 'Finance',\n",
       " 'Department',\n",
       " 'Senior',\n",
       " 'Fund',\n",
       " 'Accountant',\n",
       " 'Assistant',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'Valuated',\n",
       " 'monthly',\n",
       " 'Cayman',\n",
       " 'fund',\n",
       " 'SFC',\n",
       " 'funds',\n",
       " 'RQFII',\n",
       " 'QDII',\n",
       " 'funds',\n",
       " 'holding',\n",
       " 'types',\n",
       " 'financial',\n",
       " 'Sep',\n",
       " '2017',\n",
       " 'instruments',\n",
       " 'including',\n",
       " 'limited',\n",
       " 'stocks',\n",
       " 'options',\n",
       " 'futures',\n",
       " 'warrants',\n",
       " 'Cooperated',\n",
       " 'teammates',\n",
       " 'involving',\n",
       " 'trade',\n",
       " 'settlements',\n",
       " 'corporate',\n",
       " 'actions',\n",
       " 'price',\n",
       " 'movements',\n",
       " 'reconciling',\n",
       " 'Bloomberg',\n",
       " 'credible',\n",
       " 'sources',\n",
       " 'Coordinated',\n",
       " 'fund',\n",
       " 'managers',\n",
       " 'custodians',\n",
       " 'bankers',\n",
       " 'resolve',\n",
       " 'valuation',\n",
       " 'fund',\n",
       " 'setup',\n",
       " 'issues',\n",
       " 'Monitored',\n",
       " 'investment',\n",
       " 'position',\n",
       " 'margin',\n",
       " 'requirement',\n",
       " 'cash',\n",
       " 'flow',\n",
       " 'investment',\n",
       " 'compliance',\n",
       " 'Prepared',\n",
       " 'management',\n",
       " 'accounts',\n",
       " 'audit',\n",
       " 'queries',\n",
       " 'Coached',\n",
       " 'junior',\n",
       " 'colleagues',\n",
       " 'job',\n",
       " 'duties',\n",
       " 'job',\n",
       " 'rotations',\n",
       " 'Steven',\n",
       " 'Cheung',\n",
       " 'Co',\n",
       " 'audit',\n",
       " 'Trainee',\n",
       " 'Dec',\n",
       " '2014',\n",
       " 'Performed',\n",
       " 'audit',\n",
       " 'fieldwork',\n",
       " 'constructed',\n",
       " 'working',\n",
       " 'papers',\n",
       " 'data',\n",
       " 'received',\n",
       " 'clients',\n",
       " 'companies',\n",
       " 'Jan',\n",
       " '2015',\n",
       " 'HKUST',\n",
       " 'Department',\n",
       " 'Finance',\n",
       " 'Finance',\n",
       " 'Research',\n",
       " 'Assistant',\n",
       " 'Part-time',\n",
       " 'Apr-May',\n",
       " 'Assisted',\n",
       " 'finance',\n",
       " 'professor',\n",
       " 'collecting',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " 'research',\n",
       " 'projects',\n",
       " '2014',\n",
       " 'related',\n",
       " '100',\n",
       " 'targeted',\n",
       " 'family',\n",
       " 'firms',\n",
       " \"SEC's\",\n",
       " 'archives',\n",
       " 'Consolidated',\n",
       " 'analyzed',\n",
       " 'relevant',\n",
       " 'data',\n",
       " 'Excel',\n",
       " 'Extra-Curricular',\n",
       " 'Activities',\n",
       " 'EY',\n",
       " 'Young',\n",
       " 'Tax',\n",
       " 'Professional',\n",
       " 'Year',\n",
       " '2015',\n",
       " 'organized',\n",
       " 'EY',\n",
       " 'Finalist',\n",
       " 'Mar',\n",
       " '2015',\n",
       " 'Was',\n",
       " 'examined',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'fundamentals',\n",
       " 'international',\n",
       " 'tax',\n",
       " 'debated',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'issue',\n",
       " 'ETF',\n",
       " 'Stock',\n",
       " 'Investment',\n",
       " 'Simulation',\n",
       " 'Game',\n",
       " '2014',\n",
       " 'co-organized',\n",
       " 'HKEX',\n",
       " '23',\n",
       " 'sponsors',\n",
       " 'Nov',\n",
       " '2014',\n",
       " 'Stimulated',\n",
       " 'investment',\n",
       " 'designated',\n",
       " 'Exchange',\n",
       " 'Traded',\n",
       " 'Funds',\n",
       " 'ETFs',\n",
       " 'listed',\n",
       " 'traded',\n",
       " 'SEHK',\n",
       " 'stocks',\n",
       " 'included',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'Index',\n",
       " 'HSI',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'China',\n",
       " 'Enterprises',\n",
       " 'Index',\n",
       " 'HKSCI',\n",
       " 'Allocated',\n",
       " 'investment',\n",
       " 'prescribed',\n",
       " 'ratio',\n",
       " 'required',\n",
       " 're-build',\n",
       " 'portfolio',\n",
       " 'week',\n",
       " '4',\n",
       " 'weeks',\n",
       " 'Gained',\n",
       " 'knowledge',\n",
       " 'insights',\n",
       " 'ETF',\n",
       " 'stock',\n",
       " 'trading',\n",
       " 'simulation',\n",
       " 'Corporate',\n",
       " 'Governance',\n",
       " 'Paper',\n",
       " 'Competition',\n",
       " 'Presentation',\n",
       " 'Award',\n",
       " '2014',\n",
       " 'organized',\n",
       " 'HKICS',\n",
       " 'Finalist',\n",
       " 'Sep',\n",
       " '2014',\n",
       " 'Wrote',\n",
       " 'paper',\n",
       " 'gave',\n",
       " 'presentation',\n",
       " 'panel',\n",
       " 'judges',\n",
       " 'corporate',\n",
       " 'governance',\n",
       " 'theme',\n",
       " 'Changing',\n",
       " 'Rules',\n",
       " 'Changing',\n",
       " 'Roles',\n",
       " 'Managing',\n",
       " 'It',\n",
       " 'All',\n",
       " 'Navigate',\n",
       " 'My',\n",
       " 'Financial',\n",
       " 'Future',\n",
       " 'Case',\n",
       " 'Competition',\n",
       " 'organized',\n",
       " 'AIESEC',\n",
       " 'Hong',\n",
       " 'Kong',\n",
       " 'VISA',\n",
       " 'Finalist',\n",
       " 'Jun',\n",
       " '2013',\n",
       " 'Led',\n",
       " 'coordinated',\n",
       " 'team',\n",
       " 'wrote',\n",
       " 'proposal',\n",
       " 'aiming',\n",
       " 'improving',\n",
       " 'financial',\n",
       " 'knowledge',\n",
       " 'related',\n",
       " 'management',\n",
       " 'skills',\n",
       " 'senior',\n",
       " 'secondary',\n",
       " 'school',\n",
       " 'students',\n",
       " 'Made',\n",
       " 'video',\n",
       " 'illustrating',\n",
       " 'proposal',\n",
       " 'worked',\n",
       " 'Visual',\n",
       " 'Basic',\n",
       " 'Professional',\n",
       " 'Qualification',\n",
       " 'CFA',\n",
       " 'Level',\n",
       " 'II',\n",
       " 'Candidate',\n",
       " 'HKSI',\n",
       " 'LE',\n",
       " 'Paper',\n",
       " '1',\n",
       " 'HKICPA',\n",
       " 'QP',\n",
       " 'Paper',\n",
       " 'B',\n",
       " 'C',\n",
       " 'Languages',\n",
       " 'Computer',\n",
       " 'Skills',\n",
       " 'Knowledge',\n",
       " 'Bloomberg',\n",
       " 'proficient',\n",
       " 'Excel',\n",
       " 'Word',\n",
       " 'Proficient',\n",
       " 'English',\n",
       " 'Mandarin',\n",
       " 'Native',\n",
       " 'Cantonese',\n",
       " 'Current',\n",
       " 'salary',\n",
       " 'HKD',\n",
       " '14',\n",
       " '000',\n",
       " 'Expected',\n",
       " 'Salary',\n",
       " 'HKD',\n",
       " '18',\n",
       " '000']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for k,v in tokenized_resumes.items(): # removing stopwords\n",
    "    tokenized_resumes[k] = [word for word in v if word not in stopwords]\n",
    "tokenized_resumes['resume_(100).txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Removal of Stopwords \n",
      "\n",
      "Length of word list:  106595\n",
      "Length of vocabulary list:  16245\n",
      "Lexical diversity:  6.561711295783318\n"
     ]
    }
   ],
   "source": [
    "#Printing statistics  \n",
    "words = list(chain.from_iterable(tokenized_resumes.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print('After Removal of Stopwords \\n')\n",
    "print('Length of word list: ',len(words))\n",
    "print('Length of vocabulary list: ',len(vocab))\n",
    "print('Lexical diversity: ',lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fetching top 200 Bigrams\n",
    "\n",
    "Bigrams are pair of consecutive words. We are here to find the top 200 most frequent bigrams. \n",
    "\n",
    "The first step is to concatenate all the tokenized words using the chain.frome_iterable function. The returned list \n",
    "by the function contains a list of all the words seprated by while space.\n",
    "\n",
    "<font size=3 color=\"blue\">Why am I finding bigrams after removal of stopwords and  not after stemming?</font><br/>\n",
    "\n",
    "The reason for doing this is if bigrams were found before stopwords removal, “of-the”,\"along-with\" etc; would appear as most frequent bigram. Even though its meaning full word it doesn’t convey anything for the analysis as it would be present in most of the text.<br/>\n",
    "\n",
    "If we do it after stemming some of the words may get shortened and may become meaningless.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hong', 'Kong'),\n",
       " ('financial', 'statements'),\n",
       " ('due', 'diligence'),\n",
       " ('real', 'estate'),\n",
       " ('Pte', 'Ltd'),\n",
       " ('private', 'equity'),\n",
       " ('Asset', 'Management'),\n",
       " ('Asia', 'Pacific'),\n",
       " ('Financial', 'Services'),\n",
       " ('Microsoft', 'Office'),\n",
       " ('English', 'Mandarin'),\n",
       " ('asset', 'management'),\n",
       " ('Business', 'Administration'),\n",
       " ('cash', 'flow'),\n",
       " ('Fluent', 'English'),\n",
       " ('Real', 'Estate'),\n",
       " ('Private', 'Equity'),\n",
       " ('hedge', 'funds'),\n",
       " ('Bachelor', 'Business'),\n",
       " ('Business', 'School'),\n",
       " ('WORK', 'EXPERIENCE'),\n",
       " ('University', 'Hong'),\n",
       " ('Financial', 'Reporting'),\n",
       " ('internal', 'control'),\n",
       " ('team', 'members'),\n",
       " ('M', 'A'),\n",
       " ('Fund', 'Accountant'),\n",
       " ('Senior', 'Associate'),\n",
       " ('fund', 'managers'),\n",
       " ('Certified', 'Public'),\n",
       " ('Excel', 'PowerPoint'),\n",
       " ('financial', 'reporting'),\n",
       " ('University', 'Singapore'),\n",
       " ('listed', 'companies'),\n",
       " ('New', 'York'),\n",
       " ('Nanyang', 'Technological'),\n",
       " ('Fund', 'Services'),\n",
       " ('Assistant', 'Manager'),\n",
       " ('Vice', 'President'),\n",
       " ('English', 'Chinese'),\n",
       " ('Technological', 'University'),\n",
       " ('financial', 'models'),\n",
       " ('Ernst', 'Young'),\n",
       " ('regulatory', 'requirements'),\n",
       " ('internal', 'controls'),\n",
       " ('P', 'L'),\n",
       " ('financial', 'analysis'),\n",
       " ('Accounting', 'Finance'),\n",
       " ('Singapore', 'Pte'),\n",
       " ('U', 'S'),\n",
       " ('business', 'development'),\n",
       " ('Word', 'Excel'),\n",
       " ('Junior', 'College'),\n",
       " ('Holdings', 'Limited'),\n",
       " ('financial', 'services'),\n",
       " ('Secondary', 'School'),\n",
       " ('senior', 'management'),\n",
       " ('Investment', 'Banking'),\n",
       " ('financial', 'reports'),\n",
       " ('Deloitte', 'Touche'),\n",
       " ('Chartered', 'Accountant'),\n",
       " ('Co', 'Ltd'),\n",
       " ('internal', 'external'),\n",
       " ('PROFESSIONAL', 'EXPERIENCE'),\n",
       " ('financial', 'statement'),\n",
       " ('audit', 'engagements'),\n",
       " ('Kuala', 'Lumpur'),\n",
       " ('financial', 'institutions'),\n",
       " ('Proficient', 'Microsoft'),\n",
       " ('hedge', 'fund'),\n",
       " ('Bachelor', 'Science'),\n",
       " ('Reporting', 'Standards'),\n",
       " ('Class', 'Honours'),\n",
       " ('Work', 'Experience'),\n",
       " ('N', 'A'),\n",
       " ('years', 'experience'),\n",
       " ('US', 'GAAP'),\n",
       " ('Bachelor', 'Accountancy'),\n",
       " ('Services', 'Singapore'),\n",
       " ('corporate', 'secretarial'),\n",
       " ('Corporate', 'Secretarial'),\n",
       " ('MS', 'Office'),\n",
       " ('Stock', 'Exchange'),\n",
       " ('Equity', 'Research'),\n",
       " ('Nanyang', 'Business'),\n",
       " ('Financial', 'Adviser'),\n",
       " ('Investment', 'Management'),\n",
       " ('O', 'N'),\n",
       " ('Office', 'Word'),\n",
       " ('Word', 'PowerPoint'),\n",
       " ('Risk', 'Management'),\n",
       " ('spoken', 'written'),\n",
       " ('Kong', 'Institute'),\n",
       " ('Institute', 'Certified'),\n",
       " ('National', 'University'),\n",
       " ('City', 'University'),\n",
       " ('fund', 'management'),\n",
       " ('CFA', 'Level'),\n",
       " ('University', 'Bachelor'),\n",
       " ('Microsoft', 'Excel'),\n",
       " ('Chartered', 'Accountants'),\n",
       " ('Corporate', 'Finance'),\n",
       " ('equity', 'funds'),\n",
       " ('Greater', 'China'),\n",
       " ('Wealth', 'Management'),\n",
       " ('Audit', 'Associate'),\n",
       " ('valuation', 'models'),\n",
       " ('ad', 'hoc'),\n",
       " ('audit', 'procedures'),\n",
       " ('corporate', 'actions'),\n",
       " ('United', 'States'),\n",
       " ('Second', 'Class'),\n",
       " ('Group', 'Limited'),\n",
       " ('communication', 'skills'),\n",
       " ('International', 'Business'),\n",
       " ('Cantonese', 'Mandarin'),\n",
       " ('Public', 'Accountants'),\n",
       " ('Fixed', 'Income'),\n",
       " ('UNIVERSITY', 'OF'),\n",
       " ('Accountant', 'Singapore'),\n",
       " ('Financial', 'Analyst'),\n",
       " ('Touche', 'LLP'),\n",
       " ('GCE', 'A'),\n",
       " ('account', 'opening'),\n",
       " ('E', 'R'),\n",
       " ('wealth', 'management'),\n",
       " ('PowerPoint', 'Excel'),\n",
       " ('service', 'providers'),\n",
       " ('tight', 'deadlines'),\n",
       " ('valuation', 'reports'),\n",
       " ('Middle', 'East'),\n",
       " ('Shanghai', 'China'),\n",
       " ('written', 'spoken'),\n",
       " ('financial', 'data'),\n",
       " ('English', 'Fluent'),\n",
       " ('Microsoft', 'Word'),\n",
       " ('PTE', 'LTD'),\n",
       " ('management', 'accounts'),\n",
       " ('management', 'companies'),\n",
       " ('Hedge', 'Fund'),\n",
       " ('Banking', 'Finance'),\n",
       " ('I', 'O'),\n",
       " ('investment', 'management'),\n",
       " ('corporate', 'governance'),\n",
       " ('CFA', 'Institute'),\n",
       " ('Services', 'Pte'),\n",
       " ('MS', 'Excel'),\n",
       " ('annual', 'audit'),\n",
       " ('Business', 'Development'),\n",
       " ('audit', 'findings'),\n",
       " ('Net', 'Asset'),\n",
       " ('CPA', 'Australia'),\n",
       " ('Languages', 'English'),\n",
       " ('A', 'N'),\n",
       " ('The', 'University'),\n",
       " ('Master', 'Science'),\n",
       " ('management', 'reporting'),\n",
       " ('risk', 'management'),\n",
       " ('financial', 'modeling'),\n",
       " ('written', 'English'),\n",
       " ('Public', 'Accountant'),\n",
       " ('Singapore', 'Financial'),\n",
       " ('Limited', 'Hong'),\n",
       " ('management', 'company'),\n",
       " ('Date', 'Birth'),\n",
       " ('gmail', 'EDUCATION'),\n",
       " ('monthly', 'quarterly'),\n",
       " ('LLP', 'Singapore'),\n",
       " ('GCE', 'O'),\n",
       " ('Institute', 'Singapore'),\n",
       " ('I', 'N'),\n",
       " ('University', 'London'),\n",
       " ('Expected', 'Salary'),\n",
       " ('S', 'I'),\n",
       " ('Companies', 'Act'),\n",
       " ('New', 'Delhi'),\n",
       " ('Fund', 'Accounting'),\n",
       " ('School', 'Business'),\n",
       " ('regulatory', 'compliance'),\n",
       " ('Company', 'Secretary'),\n",
       " ('Senior', 'Audit'),\n",
       " ('funds', 'private'),\n",
       " ('clients', 'financial'),\n",
       " ('research', 'reports'),\n",
       " ('Audit', 'Senior'),\n",
       " ('Management', 'University'),\n",
       " ('First', 'Class'),\n",
       " ('Relationship', 'Manager'),\n",
       " ('accounting', 'system'),\n",
       " ('Association', 'Chartered'),\n",
       " ('Chartered', 'Certified'),\n",
       " ('Certified', 'Accountants'),\n",
       " ('Diploma', 'Business'),\n",
       " ('Company', 'Limited'),\n",
       " ('Mandarin', 'Cantonese'),\n",
       " ('Singapore', 'Polytechnic'),\n",
       " ('balance', 'sheet'),\n",
       " ('Institute', 'Management'),\n",
       " ('S', 'A'),\n",
       " ('United', 'Kingdom'),\n",
       " ('timely', 'manner'),\n",
       " ('team', 'player'),\n",
       " ('Bloomberg', 'Terminal'),\n",
       " ('industries', 'including'),\n",
       " ('Independent', 'Financial'),\n",
       " ('Singapore', 'Bachelor'),\n",
       " ('Delhi', 'India'),\n",
       " ('interpersonal', 'skills'),\n",
       " ('project', 'management'),\n",
       " ('The', 'Chinese'),\n",
       " ('Chinese', 'University'),\n",
       " ('High', 'School'),\n",
       " ('net', 'asset'),\n",
       " ('PROFESSIONAL', 'QUALIFICATIONS'),\n",
       " ('Finance', 'Manager'),\n",
       " ('Singapore', 'Malaysia'),\n",
       " ('audit', 'process'),\n",
       " ('based', 'Singapore'),\n",
       " ('Company', 'Secretarial'),\n",
       " ('Singapore', 'Institute'),\n",
       " ('tax', 'computation')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = ngrams(list(chain.from_iterable(tokenized_resumes.values())),n = 2) #Where n=2 represents number of words which occur together\n",
    "freq_dist_bg = FreqDist(bigrams) #FreqDist gives count of each word\n",
    "bg=freq_dist_bg.most_common(250)\n",
    "\n",
    "bi_list=[]\n",
    "for each in bg:\n",
    "    bi_list.append(each[0]) # picking only words from freq_dist dataframe\n",
    "    \n",
    "bigrams_list=[] # Considering only alphabetical bigram list\n",
    "for each in bi_list:\n",
    "    if each[0].isalpha()==True and each[1].isalpha()==True:\n",
    "        bigrams_list.append(each)\n",
    "bigrams_list        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Re-tokenize the words \n",
    "\n",
    "In previous Task we calculated Bigrams. Here we need to add those bigrams into our tokenize list and retokenize the whole list. To ensure the bigrams wont split we use MWE tokenizer to add '_' between them and we retokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'CHIN',\n",
       " 'Kwok',\n",
       " 'Ho',\n",
       " 'Mobile',\n",
       " '852-5347',\n",
       " '8575',\n",
       " 'E-mail',\n",
       " 'chinkhthomas',\n",
       " 'gmail',\n",
       " 'Education',\n",
       " 'The',\n",
       " 'Hong_Kong',\n",
       " 'University',\n",
       " 'Science',\n",
       " 'Technology',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'BBA',\n",
       " 'Finance',\n",
       " 'Professional',\n",
       " 'Accounting',\n",
       " 'Second_Class',\n",
       " 'Honors',\n",
       " 'Division',\n",
       " 'I',\n",
       " 'Work_Experience',\n",
       " 'BOCI-Prudential',\n",
       " 'Trustee',\n",
       " 'Limited',\n",
       " 'Finance',\n",
       " 'Department',\n",
       " 'Senior',\n",
       " 'Fund_Accountant',\n",
       " 'Assistant',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'Valuated',\n",
       " 'monthly',\n",
       " 'Cayman',\n",
       " 'fund',\n",
       " 'SFC',\n",
       " 'funds',\n",
       " 'RQFII',\n",
       " 'QDII',\n",
       " 'funds',\n",
       " 'holding',\n",
       " 'types',\n",
       " 'financial',\n",
       " 'Sep',\n",
       " '2017',\n",
       " 'instruments',\n",
       " 'including',\n",
       " 'limited',\n",
       " 'stocks',\n",
       " 'options',\n",
       " 'futures',\n",
       " 'warrants',\n",
       " 'Cooperated',\n",
       " 'teammates',\n",
       " 'involving',\n",
       " 'trade',\n",
       " 'settlements',\n",
       " 'corporate_actions',\n",
       " 'price',\n",
       " 'movements',\n",
       " 'reconciling',\n",
       " 'Bloomberg',\n",
       " 'credible',\n",
       " 'sources',\n",
       " 'Coordinated',\n",
       " 'fund_managers',\n",
       " 'custodians',\n",
       " 'bankers',\n",
       " 'resolve',\n",
       " 'valuation',\n",
       " 'fund',\n",
       " 'setup',\n",
       " 'issues',\n",
       " 'Monitored',\n",
       " 'investment',\n",
       " 'position',\n",
       " 'margin',\n",
       " 'requirement',\n",
       " 'cash_flow',\n",
       " 'investment',\n",
       " 'compliance',\n",
       " 'Prepared',\n",
       " 'management_accounts',\n",
       " 'audit',\n",
       " 'queries',\n",
       " 'Coached',\n",
       " 'junior',\n",
       " 'colleagues',\n",
       " 'job',\n",
       " 'duties',\n",
       " 'job',\n",
       " 'rotations',\n",
       " 'Steven',\n",
       " 'Cheung',\n",
       " 'Co',\n",
       " 'audit',\n",
       " 'Trainee',\n",
       " 'Dec',\n",
       " '2014',\n",
       " 'Performed',\n",
       " 'audit',\n",
       " 'fieldwork',\n",
       " 'constructed',\n",
       " 'working',\n",
       " 'papers',\n",
       " 'data',\n",
       " 'received',\n",
       " 'clients',\n",
       " 'companies',\n",
       " 'Jan',\n",
       " '2015',\n",
       " 'HKUST',\n",
       " 'Department',\n",
       " 'Finance',\n",
       " 'Finance',\n",
       " 'Research',\n",
       " 'Assistant',\n",
       " 'Part-time',\n",
       " 'Apr-May',\n",
       " 'Assisted',\n",
       " 'finance',\n",
       " 'professor',\n",
       " 'collecting',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " 'research',\n",
       " 'projects',\n",
       " '2014',\n",
       " 'related',\n",
       " '100',\n",
       " 'targeted',\n",
       " 'family',\n",
       " 'firms',\n",
       " \"SEC's\",\n",
       " 'archives',\n",
       " 'Consolidated',\n",
       " 'analyzed',\n",
       " 'relevant',\n",
       " 'data',\n",
       " 'Excel',\n",
       " 'Extra-Curricular',\n",
       " 'Activities',\n",
       " 'EY',\n",
       " 'Young',\n",
       " 'Tax',\n",
       " 'Professional',\n",
       " 'Year',\n",
       " '2015',\n",
       " 'organized',\n",
       " 'EY',\n",
       " 'Finalist',\n",
       " 'Mar',\n",
       " '2015',\n",
       " 'Was',\n",
       " 'examined',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'fundamentals',\n",
       " 'international',\n",
       " 'tax',\n",
       " 'debated',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'issue',\n",
       " 'ETF',\n",
       " 'Stock',\n",
       " 'Investment',\n",
       " 'Simulation',\n",
       " 'Game',\n",
       " '2014',\n",
       " 'co-organized',\n",
       " 'HKEX',\n",
       " '23',\n",
       " 'sponsors',\n",
       " 'Nov',\n",
       " '2014',\n",
       " 'Stimulated',\n",
       " 'investment',\n",
       " 'designated',\n",
       " 'Exchange',\n",
       " 'Traded',\n",
       " 'Funds',\n",
       " 'ETFs',\n",
       " 'listed',\n",
       " 'traded',\n",
       " 'SEHK',\n",
       " 'stocks',\n",
       " 'included',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'Index',\n",
       " 'HSI',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'China',\n",
       " 'Enterprises',\n",
       " 'Index',\n",
       " 'HKSCI',\n",
       " 'Allocated',\n",
       " 'investment',\n",
       " 'prescribed',\n",
       " 'ratio',\n",
       " 'required',\n",
       " 're-build',\n",
       " 'portfolio',\n",
       " 'week',\n",
       " '4',\n",
       " 'weeks',\n",
       " 'Gained',\n",
       " 'knowledge',\n",
       " 'insights',\n",
       " 'ETF',\n",
       " 'stock',\n",
       " 'trading',\n",
       " 'simulation',\n",
       " 'Corporate',\n",
       " 'Governance',\n",
       " 'Paper',\n",
       " 'Competition',\n",
       " 'Presentation',\n",
       " 'Award',\n",
       " '2014',\n",
       " 'organized',\n",
       " 'HKICS',\n",
       " 'Finalist',\n",
       " 'Sep',\n",
       " '2014',\n",
       " 'Wrote',\n",
       " 'paper',\n",
       " 'gave',\n",
       " 'presentation',\n",
       " 'panel',\n",
       " 'judges',\n",
       " 'corporate_governance',\n",
       " 'theme',\n",
       " 'Changing',\n",
       " 'Rules',\n",
       " 'Changing',\n",
       " 'Roles',\n",
       " 'Managing',\n",
       " 'It',\n",
       " 'All',\n",
       " 'Navigate',\n",
       " 'My',\n",
       " 'Financial',\n",
       " 'Future',\n",
       " 'Case',\n",
       " 'Competition',\n",
       " 'organized',\n",
       " 'AIESEC',\n",
       " 'Hong_Kong',\n",
       " 'VISA',\n",
       " 'Finalist',\n",
       " 'Jun',\n",
       " '2013',\n",
       " 'Led',\n",
       " 'coordinated',\n",
       " 'team',\n",
       " 'wrote',\n",
       " 'proposal',\n",
       " 'aiming',\n",
       " 'improving',\n",
       " 'financial',\n",
       " 'knowledge',\n",
       " 'related',\n",
       " 'management',\n",
       " 'skills',\n",
       " 'senior',\n",
       " 'secondary',\n",
       " 'school',\n",
       " 'students',\n",
       " 'Made',\n",
       " 'video',\n",
       " 'illustrating',\n",
       " 'proposal',\n",
       " 'worked',\n",
       " 'Visual',\n",
       " 'Basic',\n",
       " 'Professional',\n",
       " 'Qualification',\n",
       " 'CFA_Level',\n",
       " 'II',\n",
       " 'Candidate',\n",
       " 'HKSI',\n",
       " 'LE',\n",
       " 'Paper',\n",
       " '1',\n",
       " 'HKICPA',\n",
       " 'QP',\n",
       " 'Paper',\n",
       " 'B',\n",
       " 'C',\n",
       " 'Languages',\n",
       " 'Computer',\n",
       " 'Skills',\n",
       " 'Knowledge',\n",
       " 'Bloomberg',\n",
       " 'proficient',\n",
       " 'Excel',\n",
       " 'Word',\n",
       " 'Proficient',\n",
       " 'English_Mandarin',\n",
       " 'Native',\n",
       " 'Cantonese',\n",
       " 'Current',\n",
       " 'salary',\n",
       " 'HKD',\n",
       " '14',\n",
       " '000',\n",
       " 'Expected_Salary',\n",
       " 'HKD',\n",
       " '18',\n",
       " '000']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#passing it to MWE Tokenizer to re-tokenize and add '_' between words\n",
    "mwetokenizer = MWETokenizer(bigrams_list)\n",
    "tokenized_resumes =  dict((k, mwetokenizer.tokenize(v)) for k,v in tokenized_resumes.items())\n",
    "tokenized_resumes['resume_(100).txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Adding Bi-Grams back to tokenlist \n",
      "\n",
      "Size of word list:  101727\n",
      "Size of vocabulary list:  16456\n",
      "lexical diversity:  6.181757413709286\n"
     ]
    }
   ],
   "source": [
    "# Getting Statistics of the words\n",
    "words = list(chain.from_iterable(tokenized_resumes.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print('After Adding Bi-Grams back to tokenlist \\n')\n",
    "print('Size of word list: ',len(words))\n",
    "print('Size of vocabulary list: ',len(vocab))\n",
    "print('lexical diversity: ',lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stemming\n",
    "\n",
    "The idea of stemming is a kind of normalization . Words have different variations but carry the same meaning. \n",
    "\n",
    "<font size=3 color=\"blue\">Why are we doing stemming here ? Why only on lowercase characters for stemming?</font>\n",
    "\n",
    "The reason why we stem is to shorten the lookup, and normalize sentences. When all he words are normalised it is easier to analyse and conclude.\n",
    "\n",
    "When you pass tokens to Porter stemmer, it not only results in stemmed words but also converts everything into lower case. This something which is not expected. We may loose information. \tHence, we passing only lower tokens would give us the necessary results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'CHIN',\n",
       " 'Kwok',\n",
       " 'Ho',\n",
       " 'Mobile',\n",
       " '852-5347',\n",
       " '8575',\n",
       " 'E-mail',\n",
       " 'chinkhthomas',\n",
       " 'gmail',\n",
       " 'Education',\n",
       " 'The',\n",
       " 'Hong_Kong',\n",
       " 'University',\n",
       " 'Science',\n",
       " 'Technology',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'BBA',\n",
       " 'Finance',\n",
       " 'Professional',\n",
       " 'Accounting',\n",
       " 'Second_Class',\n",
       " 'Honors',\n",
       " 'Division',\n",
       " 'I',\n",
       " 'Work_Experience',\n",
       " 'BOCI-Prudential',\n",
       " 'Trustee',\n",
       " 'Limited',\n",
       " 'Finance',\n",
       " 'Department',\n",
       " 'Senior',\n",
       " 'Fund_Accountant',\n",
       " 'Assistant',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'Valuated',\n",
       " 'monthly',\n",
       " 'Cayman',\n",
       " 'fund',\n",
       " 'SFC',\n",
       " 'funds',\n",
       " 'RQFII',\n",
       " 'QDII',\n",
       " 'funds',\n",
       " 'holding',\n",
       " 'types',\n",
       " 'financial',\n",
       " 'Sep',\n",
       " '2017',\n",
       " 'instruments',\n",
       " 'including',\n",
       " 'limited',\n",
       " 'stocks',\n",
       " 'options',\n",
       " 'futures',\n",
       " 'warrants',\n",
       " 'Cooperated',\n",
       " 'teammates',\n",
       " 'involving',\n",
       " 'trade',\n",
       " 'settlements',\n",
       " 'corporate_actions',\n",
       " 'price',\n",
       " 'movements',\n",
       " 'reconciling',\n",
       " 'Bloomberg',\n",
       " 'credible',\n",
       " 'sources',\n",
       " 'Coordinated',\n",
       " 'fund_managers',\n",
       " 'custodians',\n",
       " 'bankers',\n",
       " 'resolve',\n",
       " 'valuation',\n",
       " 'fund',\n",
       " 'setup',\n",
       " 'issues',\n",
       " 'Monitored',\n",
       " 'investment',\n",
       " 'position',\n",
       " 'margin',\n",
       " 'requirement',\n",
       " 'cash_flow',\n",
       " 'investment',\n",
       " 'compliance',\n",
       " 'Prepared',\n",
       " 'management_accounts',\n",
       " 'audit',\n",
       " 'queries',\n",
       " 'Coached',\n",
       " 'junior',\n",
       " 'colleagues',\n",
       " 'job',\n",
       " 'duties',\n",
       " 'job',\n",
       " 'rotations',\n",
       " 'Steven',\n",
       " 'Cheung',\n",
       " 'Co',\n",
       " 'audit',\n",
       " 'Trainee',\n",
       " 'Dec',\n",
       " '2014',\n",
       " 'Performed',\n",
       " 'audit',\n",
       " 'fieldwork',\n",
       " 'constructed',\n",
       " 'working',\n",
       " 'papers',\n",
       " 'data',\n",
       " 'received',\n",
       " 'clients',\n",
       " 'companies',\n",
       " 'Jan',\n",
       " '2015',\n",
       " 'HKUST',\n",
       " 'Department',\n",
       " 'Finance',\n",
       " 'Finance',\n",
       " 'Research',\n",
       " 'Assistant',\n",
       " 'Part-time',\n",
       " 'Apr-May',\n",
       " 'Assisted',\n",
       " 'finance',\n",
       " 'professor',\n",
       " 'collecting',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " 'research',\n",
       " 'projects',\n",
       " '2014',\n",
       " 'related',\n",
       " '100',\n",
       " 'targeted',\n",
       " 'family',\n",
       " 'firms',\n",
       " \"SEC's\",\n",
       " 'archives',\n",
       " 'Consolidated',\n",
       " 'analyzed',\n",
       " 'relevant',\n",
       " 'data',\n",
       " 'Excel',\n",
       " 'Extra-Curricular',\n",
       " 'Activities',\n",
       " 'EY',\n",
       " 'Young',\n",
       " 'Tax',\n",
       " 'Professional',\n",
       " 'Year',\n",
       " '2015',\n",
       " 'organized',\n",
       " 'EY',\n",
       " 'Finalist',\n",
       " 'Mar',\n",
       " '2015',\n",
       " 'Was',\n",
       " 'examined',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'fundamentals',\n",
       " 'international',\n",
       " 'tax',\n",
       " 'debated',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'issue',\n",
       " 'ETF',\n",
       " 'Stock',\n",
       " 'Investment',\n",
       " 'Simulation',\n",
       " 'Game',\n",
       " '2014',\n",
       " 'co-organized',\n",
       " 'HKEX',\n",
       " '23',\n",
       " 'sponsors',\n",
       " 'Nov',\n",
       " '2014',\n",
       " 'Stimulated',\n",
       " 'investment',\n",
       " 'designated',\n",
       " 'Exchange',\n",
       " 'Traded',\n",
       " 'Funds',\n",
       " 'ETFs',\n",
       " 'listed',\n",
       " 'traded',\n",
       " 'SEHK',\n",
       " 'stocks',\n",
       " 'included',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'Index',\n",
       " 'HSI',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'China',\n",
       " 'Enterprises',\n",
       " 'Index',\n",
       " 'HKSCI',\n",
       " 'Allocated',\n",
       " 'investment',\n",
       " 'prescribed',\n",
       " 'ratio',\n",
       " 'required',\n",
       " 're-build',\n",
       " 'portfolio',\n",
       " 'week',\n",
       " '4',\n",
       " 'weeks',\n",
       " 'Gained',\n",
       " 'knowledge',\n",
       " 'insights',\n",
       " 'ETF',\n",
       " 'stock',\n",
       " 'trading',\n",
       " 'simulation',\n",
       " 'Corporate',\n",
       " 'Governance',\n",
       " 'Paper',\n",
       " 'Competition',\n",
       " 'Presentation',\n",
       " 'Award',\n",
       " '2014',\n",
       " 'organized',\n",
       " 'HKICS',\n",
       " 'Finalist',\n",
       " 'Sep',\n",
       " '2014',\n",
       " 'Wrote',\n",
       " 'paper',\n",
       " 'gave',\n",
       " 'presentation',\n",
       " 'panel',\n",
       " 'judges',\n",
       " 'corporate_governance',\n",
       " 'theme',\n",
       " 'Changing',\n",
       " 'Rules',\n",
       " 'Changing',\n",
       " 'Roles',\n",
       " 'Managing',\n",
       " 'It',\n",
       " 'All',\n",
       " 'Navigate',\n",
       " 'My',\n",
       " 'Financial',\n",
       " 'Future',\n",
       " 'Case',\n",
       " 'Competition',\n",
       " 'organized',\n",
       " 'AIESEC',\n",
       " 'Hong_Kong',\n",
       " 'VISA',\n",
       " 'Finalist',\n",
       " 'Jun',\n",
       " '2013',\n",
       " 'Led',\n",
       " 'coordinated',\n",
       " 'team',\n",
       " 'wrote',\n",
       " 'proposal',\n",
       " 'aiming',\n",
       " 'improving',\n",
       " 'financial',\n",
       " 'knowledge',\n",
       " 'related',\n",
       " 'management',\n",
       " 'skills',\n",
       " 'senior',\n",
       " 'secondary',\n",
       " 'school',\n",
       " 'students',\n",
       " 'Made',\n",
       " 'video',\n",
       " 'illustrating',\n",
       " 'proposal',\n",
       " 'worked',\n",
       " 'Visual',\n",
       " 'Basic',\n",
       " 'Professional',\n",
       " 'Qualification',\n",
       " 'CFA_Level',\n",
       " 'II',\n",
       " 'Candidate',\n",
       " 'HKSI',\n",
       " 'LE',\n",
       " 'Paper',\n",
       " '1',\n",
       " 'HKICPA',\n",
       " 'QP',\n",
       " 'Paper',\n",
       " 'B',\n",
       " 'C',\n",
       " 'Languages',\n",
       " 'Computer',\n",
       " 'Skills',\n",
       " 'Knowledge',\n",
       " 'Bloomberg',\n",
       " 'proficient',\n",
       " 'Excel',\n",
       " 'Word',\n",
       " 'Proficient',\n",
       " 'English_Mandarin',\n",
       " 'Native',\n",
       " 'Cantonese',\n",
       " 'Current',\n",
       " 'salary',\n",
       " 'HKD',\n",
       " '14',\n",
       " '000',\n",
       " 'Expected_Salary',\n",
       " 'HKD',\n",
       " '18',\n",
       " '000']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Porter Stemmer for stemming\n",
    "stemmer = PorterStemmer()\n",
    "for k,v in tokenized_resumes.items():\n",
    "    tokenized_resumes[k] = [stemmer.stem(token) if re.search('([a-z]+[_][a-z]+)',token)==False & token.islower() else token for token in v]\n",
    "tokenized_resumes['resume_(100).txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming \n",
      "\n",
      "Size of word list:  101727\n",
      "Size of vocabulary list:  16456\n",
      "lexical diversity:  6.181757413709286\n"
     ]
    }
   ],
   "source": [
    "#Displaying Statistics\n",
    "words = list(chain.from_iterable(tokenized_resumes.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print('After Stemming \\n')\n",
    "print('Size of word list: ',len(words))\n",
    "print('Size of vocabulary list: ',len(vocab))\n",
    "print('lexical diversity: ',lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.Removal of less and more context dependant words \n",
    "\n",
    "This part invloves removal of words which are present in more than 98% of the document and less than 2% of the documents. \n",
    "\n",
    "<font size=3 color=\"blue\">Why are we doing this? and why at this stage</font>\n",
    "\n",
    "The words which appear in more than 98% documents and which are appearing in less than 2% are of no use because the probablity of determining the match is high and other case its low. So for the analysis it wont be helpfull.\n",
    "\n",
    "If the words were removed earlier we would have lost bigrams. \n",
    "\n",
    "\n",
    "Considering resumes as context, removing less frequent tokens at the end would be better. For instance, Consider two sentences, I am a critical thinker and  I am good at critical thinking, if thinking occurs in most places than thinker , rejecting thinker would make you lose one potential candidate for the interview. \n",
    "If this is done after stemming thinker and thinking would be stemmed to same word and hence we still consider him for the interview.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Ho',\n",
       " 'Mobile',\n",
       " 'E-mail',\n",
       " 'gmail',\n",
       " 'Education',\n",
       " 'The',\n",
       " 'Hong_Kong',\n",
       " 'University',\n",
       " 'Science',\n",
       " 'Technology',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'BBA',\n",
       " 'Finance',\n",
       " 'Professional',\n",
       " 'Accounting',\n",
       " 'Second_Class',\n",
       " 'Honors',\n",
       " 'Division',\n",
       " 'I',\n",
       " 'Work_Experience',\n",
       " 'Trustee',\n",
       " 'Limited',\n",
       " 'Finance',\n",
       " 'Department',\n",
       " 'Senior',\n",
       " 'Fund_Accountant',\n",
       " 'Assistant',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'monthly',\n",
       " 'Cayman',\n",
       " 'fund',\n",
       " 'SFC',\n",
       " 'funds',\n",
       " 'funds',\n",
       " 'holding',\n",
       " 'types',\n",
       " 'financial',\n",
       " 'Sep',\n",
       " '2017',\n",
       " 'instruments',\n",
       " 'including',\n",
       " 'limited',\n",
       " 'stocks',\n",
       " 'options',\n",
       " 'futures',\n",
       " 'teammates',\n",
       " 'involving',\n",
       " 'trade',\n",
       " 'settlements',\n",
       " 'corporate_actions',\n",
       " 'price',\n",
       " 'movements',\n",
       " 'Bloomberg',\n",
       " 'sources',\n",
       " 'Coordinated',\n",
       " 'fund_managers',\n",
       " 'custodians',\n",
       " 'bankers',\n",
       " 'resolve',\n",
       " 'valuation',\n",
       " 'fund',\n",
       " 'issues',\n",
       " 'Monitored',\n",
       " 'investment',\n",
       " 'position',\n",
       " 'margin',\n",
       " 'requirement',\n",
       " 'cash_flow',\n",
       " 'investment',\n",
       " 'compliance',\n",
       " 'Prepared',\n",
       " 'management_accounts',\n",
       " 'audit',\n",
       " 'queries',\n",
       " 'junior',\n",
       " 'colleagues',\n",
       " 'job',\n",
       " 'duties',\n",
       " 'job',\n",
       " 'Cheung',\n",
       " 'Co',\n",
       " 'audit',\n",
       " 'Trainee',\n",
       " 'Dec',\n",
       " '2014',\n",
       " 'Performed',\n",
       " 'audit',\n",
       " 'fieldwork',\n",
       " 'working',\n",
       " 'papers',\n",
       " 'data',\n",
       " 'received',\n",
       " 'clients',\n",
       " 'companies',\n",
       " 'Jan',\n",
       " '2015',\n",
       " 'Department',\n",
       " 'Finance',\n",
       " 'Finance',\n",
       " 'Research',\n",
       " 'Assistant',\n",
       " 'Part-time',\n",
       " 'Assisted',\n",
       " 'finance',\n",
       " 'collecting',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " 'research',\n",
       " 'projects',\n",
       " '2014',\n",
       " 'related',\n",
       " '100',\n",
       " 'targeted',\n",
       " 'family',\n",
       " 'firms',\n",
       " 'analyzed',\n",
       " 'relevant',\n",
       " 'data',\n",
       " 'Excel',\n",
       " 'Activities',\n",
       " 'EY',\n",
       " 'Young',\n",
       " 'Tax',\n",
       " 'Professional',\n",
       " 'Year',\n",
       " '2015',\n",
       " 'organized',\n",
       " 'EY',\n",
       " 'Finalist',\n",
       " 'Mar',\n",
       " '2015',\n",
       " 'examined',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'international',\n",
       " 'tax',\n",
       " 'HK',\n",
       " 'tax',\n",
       " 'issue',\n",
       " 'Stock',\n",
       " 'Investment',\n",
       " '2014',\n",
       " '23',\n",
       " 'Nov',\n",
       " '2014',\n",
       " 'investment',\n",
       " 'Exchange',\n",
       " 'Funds',\n",
       " 'listed',\n",
       " 'traded',\n",
       " 'stocks',\n",
       " 'included',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'Index',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'China',\n",
       " 'Index',\n",
       " 'investment',\n",
       " 'prescribed',\n",
       " 'ratio',\n",
       " 'required',\n",
       " 'portfolio',\n",
       " 'week',\n",
       " '4',\n",
       " 'weeks',\n",
       " 'Gained',\n",
       " 'knowledge',\n",
       " 'insights',\n",
       " 'stock',\n",
       " 'trading',\n",
       " 'Corporate',\n",
       " 'Governance',\n",
       " 'Paper',\n",
       " 'Competition',\n",
       " 'Presentation',\n",
       " 'Award',\n",
       " '2014',\n",
       " 'organized',\n",
       " 'Finalist',\n",
       " 'Sep',\n",
       " '2014',\n",
       " 'Wrote',\n",
       " 'paper',\n",
       " 'gave',\n",
       " 'presentation',\n",
       " 'corporate_governance',\n",
       " 'Rules',\n",
       " 'Roles',\n",
       " 'Managing',\n",
       " 'All',\n",
       " 'My',\n",
       " 'Financial',\n",
       " 'Case',\n",
       " 'Competition',\n",
       " 'organized',\n",
       " 'Hong_Kong',\n",
       " 'Finalist',\n",
       " 'Jun',\n",
       " '2013',\n",
       " 'Led',\n",
       " 'coordinated',\n",
       " 'team',\n",
       " 'wrote',\n",
       " 'proposal',\n",
       " 'improving',\n",
       " 'financial',\n",
       " 'knowledge',\n",
       " 'related',\n",
       " 'management',\n",
       " 'skills',\n",
       " 'senior',\n",
       " 'secondary',\n",
       " 'school',\n",
       " 'students',\n",
       " 'Made',\n",
       " 'video',\n",
       " 'proposal',\n",
       " 'worked',\n",
       " 'Visual',\n",
       " 'Basic',\n",
       " 'Professional',\n",
       " 'Qualification',\n",
       " 'CFA_Level',\n",
       " 'II',\n",
       " 'Candidate',\n",
       " 'Paper',\n",
       " '1',\n",
       " 'HKICPA',\n",
       " 'Paper',\n",
       " 'B',\n",
       " 'C',\n",
       " 'Languages',\n",
       " 'Computer',\n",
       " 'Skills',\n",
       " 'Knowledge',\n",
       " 'Bloomberg',\n",
       " 'proficient',\n",
       " 'Excel',\n",
       " 'Word',\n",
       " 'Proficient',\n",
       " 'English_Mandarin',\n",
       " 'Native',\n",
       " 'Cantonese',\n",
       " 'Current',\n",
       " 'salary',\n",
       " 'HKD',\n",
       " '14',\n",
       " '000',\n",
       " 'Expected_Salary',\n",
       " 'HKD',\n",
       " '18',\n",
       " '000']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the frequency of each token using FreqDist  \n",
    "less_freq_tokens=[]\n",
    "for k,v in tokenized_resumes.items():\n",
    "    less_freq_tokens+=list(set(v))\n",
    "word_freq = FreqDist(less_freq_tokens)    \n",
    "\n",
    "#Less than 2%\n",
    "remove_list_2=[]   \n",
    "for k,v in word_freq.items():\n",
    "    if v/len(resume_names)<0.02:\n",
    "        remove_list_2.append(k)\n",
    "#greater than 98%\n",
    "remove_list_98=[]   \n",
    "for k,v in word_freq.items():\n",
    "    if v/len(resume_names)>0.98:\n",
    "        remove_list_98.append(k)\n",
    "\n",
    "remove_list=remove_list_2+remove_list_98\n",
    "\n",
    "# removing both the 98% and 2% lists\n",
    "for k,v in tokenized_resumes.items():\n",
    "    tokenized_resumes[k] = [word for word in v if word not in remove_list]\n",
    "\n",
    "tokenized_resumes['resume_(100).txt']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing context dependant words \n",
      "\n",
      "Size of word list:  78155\n",
      "Size of vocabulary list:  3119\n",
      "lexical diversity:  25.05771080474511\n"
     ]
    }
   ],
   "source": [
    "#Displaying the statistics \n",
    "words = list(chain.from_iterable(tokenized_resumes.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print('After removing context dependant words \\n')\n",
    "print('Size of word list: ',len(words))\n",
    "print('Size of vocabulary list: ',len(vocab))\n",
    "print('lexical diversity: ',lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.Removal of words which have length less than 3\n",
    "\n",
    "This step can be done anywhere before stemming or after stemming. Just to have final each vocabulary word length greater than 3 considering it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name',\n",
       " 'Mobile',\n",
       " 'E-mail',\n",
       " 'gmail',\n",
       " 'Education',\n",
       " 'The',\n",
       " 'Hong_Kong',\n",
       " 'University',\n",
       " 'Science',\n",
       " 'Technology',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'BBA',\n",
       " 'Finance',\n",
       " 'Professional',\n",
       " 'Accounting',\n",
       " 'Second_Class',\n",
       " 'Honors',\n",
       " 'Division',\n",
       " 'Work_Experience',\n",
       " 'Trustee',\n",
       " 'Limited',\n",
       " 'Finance',\n",
       " 'Department',\n",
       " 'Senior',\n",
       " 'Fund_Accountant',\n",
       " 'Assistant',\n",
       " 'Sep',\n",
       " '2015',\n",
       " 'monthly',\n",
       " 'Cayman',\n",
       " 'fund',\n",
       " 'SFC',\n",
       " 'funds',\n",
       " 'funds',\n",
       " 'holding',\n",
       " 'types',\n",
       " 'financial',\n",
       " 'Sep',\n",
       " '2017',\n",
       " 'instruments',\n",
       " 'including',\n",
       " 'limited',\n",
       " 'stocks',\n",
       " 'options',\n",
       " 'futures',\n",
       " 'teammates',\n",
       " 'involving',\n",
       " 'trade',\n",
       " 'settlements',\n",
       " 'corporate_actions',\n",
       " 'price',\n",
       " 'movements',\n",
       " 'Bloomberg',\n",
       " 'sources',\n",
       " 'Coordinated',\n",
       " 'fund_managers',\n",
       " 'custodians',\n",
       " 'bankers',\n",
       " 'resolve',\n",
       " 'valuation',\n",
       " 'fund',\n",
       " 'issues',\n",
       " 'Monitored',\n",
       " 'investment',\n",
       " 'position',\n",
       " 'margin',\n",
       " 'requirement',\n",
       " 'cash_flow',\n",
       " 'investment',\n",
       " 'compliance',\n",
       " 'Prepared',\n",
       " 'management_accounts',\n",
       " 'audit',\n",
       " 'queries',\n",
       " 'junior',\n",
       " 'colleagues',\n",
       " 'job',\n",
       " 'duties',\n",
       " 'job',\n",
       " 'Cheung',\n",
       " 'audit',\n",
       " 'Trainee',\n",
       " 'Dec',\n",
       " '2014',\n",
       " 'Performed',\n",
       " 'audit',\n",
       " 'fieldwork',\n",
       " 'working',\n",
       " 'papers',\n",
       " 'data',\n",
       " 'received',\n",
       " 'clients',\n",
       " 'companies',\n",
       " 'Jan',\n",
       " '2015',\n",
       " 'Department',\n",
       " 'Finance',\n",
       " 'Finance',\n",
       " 'Research',\n",
       " 'Assistant',\n",
       " 'Part-time',\n",
       " 'Assisted',\n",
       " 'finance',\n",
       " 'collecting',\n",
       " 'analyzing',\n",
       " 'data',\n",
       " 'research',\n",
       " 'projects',\n",
       " '2014',\n",
       " 'related',\n",
       " '100',\n",
       " 'targeted',\n",
       " 'family',\n",
       " 'firms',\n",
       " 'analyzed',\n",
       " 'relevant',\n",
       " 'data',\n",
       " 'Excel',\n",
       " 'Activities',\n",
       " 'Young',\n",
       " 'Tax',\n",
       " 'Professional',\n",
       " 'Year',\n",
       " '2015',\n",
       " 'organized',\n",
       " 'Finalist',\n",
       " 'Mar',\n",
       " '2015',\n",
       " 'examined',\n",
       " 'tax',\n",
       " 'international',\n",
       " 'tax',\n",
       " 'tax',\n",
       " 'issue',\n",
       " 'Stock',\n",
       " 'Investment',\n",
       " '2014',\n",
       " 'Nov',\n",
       " '2014',\n",
       " 'investment',\n",
       " 'Exchange',\n",
       " 'Funds',\n",
       " 'listed',\n",
       " 'traded',\n",
       " 'stocks',\n",
       " 'included',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'Index',\n",
       " 'Hang',\n",
       " 'Seng',\n",
       " 'China',\n",
       " 'Index',\n",
       " 'investment',\n",
       " 'prescribed',\n",
       " 'ratio',\n",
       " 'required',\n",
       " 'portfolio',\n",
       " 'week',\n",
       " 'weeks',\n",
       " 'Gained',\n",
       " 'knowledge',\n",
       " 'insights',\n",
       " 'stock',\n",
       " 'trading',\n",
       " 'Corporate',\n",
       " 'Governance',\n",
       " 'Paper',\n",
       " 'Competition',\n",
       " 'Presentation',\n",
       " 'Award',\n",
       " '2014',\n",
       " 'organized',\n",
       " 'Finalist',\n",
       " 'Sep',\n",
       " '2014',\n",
       " 'Wrote',\n",
       " 'paper',\n",
       " 'gave',\n",
       " 'presentation',\n",
       " 'corporate_governance',\n",
       " 'Rules',\n",
       " 'Roles',\n",
       " 'Managing',\n",
       " 'All',\n",
       " 'Financial',\n",
       " 'Case',\n",
       " 'Competition',\n",
       " 'organized',\n",
       " 'Hong_Kong',\n",
       " 'Finalist',\n",
       " 'Jun',\n",
       " '2013',\n",
       " 'Led',\n",
       " 'coordinated',\n",
       " 'team',\n",
       " 'wrote',\n",
       " 'proposal',\n",
       " 'improving',\n",
       " 'financial',\n",
       " 'knowledge',\n",
       " 'related',\n",
       " 'management',\n",
       " 'skills',\n",
       " 'senior',\n",
       " 'secondary',\n",
       " 'school',\n",
       " 'students',\n",
       " 'Made',\n",
       " 'video',\n",
       " 'proposal',\n",
       " 'worked',\n",
       " 'Visual',\n",
       " 'Basic',\n",
       " 'Professional',\n",
       " 'Qualification',\n",
       " 'CFA_Level',\n",
       " 'Candidate',\n",
       " 'Paper',\n",
       " 'HKICPA',\n",
       " 'Paper',\n",
       " 'Languages',\n",
       " 'Computer',\n",
       " 'Skills',\n",
       " 'Knowledge',\n",
       " 'Bloomberg',\n",
       " 'proficient',\n",
       " 'Excel',\n",
       " 'Word',\n",
       " 'Proficient',\n",
       " 'English_Mandarin',\n",
       " 'Native',\n",
       " 'Cantonese',\n",
       " 'Current',\n",
       " 'salary',\n",
       " 'HKD',\n",
       " '000',\n",
       " 'Expected_Salary',\n",
       " 'HKD',\n",
       " '000']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing words which have less than 3\n",
    "for k,v in tokenized_resumes.items():\n",
    "    tokenized_resumes[k] = [word for word in v if len(word)>=3]  \n",
    "tokenized_resumes['resume_(100).txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing words which have length less than 3 \n",
      "\n",
      "Size of word list:  72739\n",
      "Size of vocabulary list:  2991\n",
      "lexical diversity:  24.319291206954198\n"
     ]
    }
   ],
   "source": [
    "words = list(chain.from_iterable(tokenized_resumes.values()))\n",
    "vocab = set(words)\n",
    "lexical_diversity = len(words)/len(vocab)\n",
    "print('After removing words which have length less than 3 \\n')\n",
    "print('Size of word list: ',len(words))\n",
    "print('Size of vocabulary list: ',len(vocab))\n",
    "print('lexical diversity: ',lexical_diversity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Vectorizing, Indexing and Writing to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file_dict = open(\"29416000_vocab.txt\", 'w') \n",
    "vocab = list(vocab)\n",
    "\n",
    "# indexing every token\n",
    "vocab_dict = {}\n",
    "i = 0\n",
    "for w in vocab:\n",
    "    vocab_dict[w] = i\n",
    "    i = i + 1\n",
    "# Writing it to file   \n",
    "for k, v in sorted(vocab_dict.items()):\n",
    "    out_file_dict.write(\"{}:{} \".format(k,v))\n",
    "    out_file_dict.write('\\n')\n",
    "\n",
    "out_file_dict.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a Sparse Matrix\n",
    "out_file_vector = open(\"29416000_countVec.txt\", 'w')\n",
    "#Vectorizing\n",
    "for resume,token in tokenized_resumes.items():\n",
    "    out_file_vector.write(resume[:-4]+' : ')# slicing is used to consider only resume_(n) not resume_(n).txt\n",
    "    d_idx = [vocab_dict[w] for w in token]    \n",
    "    for k, v in FreqDist(d_idx).items():       \n",
    "        out_file_vector.write(\"{}:{} \".format(k,v))\n",
    "    out_file_vector.write('\\n\\n')\n",
    "    \n",
    "out_file_vector.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "This assessment measured the understanding of basic text file processing techniques in the Python programming language. The main outcomes achieved while applying these techniques were:\n",
    "\n",
    "- **Sentence Segmentation**. By using the built-in `nltk.data English pickle` module, it was possible split the sentences of the each resume.\n",
    "- **Case Normlization**. By using the `re` package, designed regex which captures only the first word of the sentence and changes to lower case.  \n",
    "- **Tokenization**. By using the `nltk` and `re` package, regular expressions had to be used to tokenize text and obtain letter-only words. Roughly 200 bigrams were also generated to further tokenize the initial corpus. The Frequency Distribution was to detect pairs of words with more frequently appearing together. In addition, bigram filters were also used to refine the bigrams even more by considering only alphabetical words.Stemming process was carried out to shoten the lookup for prediction.\n",
    "- **Vocabulary and sparse vector generation**. A vocabulary covering words from different abstracts was obtained by  removing stop words, words> 98% (most frequent ones), and words that appeared in <2% documents. Finally, a sparse vector was calculated for every abstract by counting the frequency of vocabulary word occurrences.\n",
    "\n",
    "\n",
    "######################### Text Statistics Before Wrangling ##################################\n",
    "\n",
    "Vocabulary size:  16539<br/>\n",
    "Total number of tokens:  143031<br/>\n",
    "Lexical diversity:  8.648104480319246<br/>\n",
    "\n",
    "######################### Final Text Statistics After Wranling ##################################\n",
    "\n",
    "Vocabulary size:  2990 <br/>\n",
    "Total number of tokens:  72764<br/>\n",
    "Lexical diversity:  24.335785953177258"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. References\n",
    "\n",
    "* https://docs.python.org/3/library/re.html\n",
    "* https://www.nltk.org/\n",
    "* NLTK Project. (2017). NLTK 3.0 documentation: nltk.tokenize.regexp module. Retrieved from http://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.RegexpTokenizer\n",
    "* NLTK Project. (2015). Collocations. Retrieved from http://www.nltk.org/howto/collocations.html\n",
    "* https://www.nltk.org/_modules/nltk/stem/porter.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
